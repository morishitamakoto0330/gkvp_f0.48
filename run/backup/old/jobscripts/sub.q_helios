#!/bin/bash

#SBATCH -A EMTURB         # Project name
#SBATCH -J GKV            # Job name
#SBATCH -N 256            # Number of nodes
#SBATCH -n 2048           # Number of tasks
#SBATCH -c 2
#SBATCH -o %j.out         # strour filename (%j is JobID.)
#SBATCH -e %j.err         # strerr filename
#SBATCH -t 24:00:00       # Execute time

#module load bullxmpi intel
module load intelmpi intel

ulimit -c 0
ulimit -s unlimited

export OMP_NUM_THREADS=2
export MKL_NUM_THREADS=1
export KMP_AFFINITY=verbose
export KMP_STACKSIZE=4G
echo $SLURM_JOB_ID
echo $SLURM_JOB_NUM_NODES
echo $SLURM_JOB_NODELIST
export I_MPI_EXTRA_FILESYSTEM=enable
export I_MPI_EXTRA_FILESYSTEM_LIST=lustre

NAGDIR=/csc/softs/nag/fsl6i22dc
export LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:${NAGDIR}/lib"


### Working directory 
DIR=/csc/workdir3/smaeyama/gkvp/f0.30/itgkbm/nl_beta020/
LDM=gkvp_mpifft.exe
NL=gkvp_f0.30_namelist.001


#### Run
#date
#cd ${DIR}
#export fu05=${DIR}/${NL}
#mpirun -n 256 -perhost 8 ${DIR}/${LDM}
#  # -n       "Total number of MPI processes"
#  # -perhost "Number of MPI processes per node"
#sleep 10
#touch complete
#date

### Run the binary load module on /tmp on each node
date
cd ${DIR}
export fu05=${DIR}/${NL}
BIN=${LDM}
rm -f ${BIN}.local
srun -N ${SLURM_NNODES} -n ${SLURM_NNODES} cp ${BIN} /tmp/${BIN}
ln -s /tmp/${BIN} ${BIN}.local
mpirun -n 2048 -ppn 8 ./${BIN}.local
  # -n       "Total number of MPI processes"
  # -ppn     "Number of MPI processes per node"
sleep 10
touch complete
date
